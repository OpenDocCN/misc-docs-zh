## 第2章多层网络

本章简要概述了计算机视觉中应用最突出的多层结构。值得注意的是，虽然本章涵盖了文献中最重要的贡献，但它不会对这些结构进行全面的介绍，因为这样的介绍可以在别处获得（例如参考文献[17,56,90]）。相反，本章的目的是为文档的其余部分奠定基础，并详细介绍和讨论目前对应用于视觉信息处理的卷积网络的理解。

### 2.1多层架构

在最近成功开发基于深度学习的网络之前，用于识别的最先进的计算机视觉系统依赖于两个独立但互补的步骤。首先，通过一组手工设计的操作（例如。具有基组，局部或全局编码方法的卷积）将输入数据转换成合适的形式。输入引起的变换通常需要找到输入数据的紧凑和/或抽象表示，同时根据手头的任务注入若干不变性。这种转换的目标是以一种更容易被分类器分开的方式改变数据。其次，变换后的数据用于训练某种分类器（例如支持向量机）来识别输入信号的内容。使用的任何分类器的性能通常都会受到使用的转换的严重影响。

具有学习的多层体系结构通过提出不仅仅使用分类器来学习，而且还直接从数据中学习所需的转换操作，从而对问题产生了不同的展望。这种学习形式通常被称为表示学习[7,90]，当在深层多层体系结构的前后层结构中使用时，称为深度学习。

多层体系结构可以定义为允许从输入数据中提取多个抽象级别有用信息的计算模型。通常，多层架构被设计为放大较高层输入的重要成分，同时对较不显着的变化变得越来越稳健。大多数多层架构交替使用线性和非线性函数堆叠简单构建模块。多年来，学者们提出了多种多样的多层架构，本节将介绍计算机视觉应用中最突出的这种架构。特别是由于显著性，人工神经网络架构将成为焦点。为了简洁起见，下面将更简单地将这种网络称为神经网络。

#### 2.1.1神经网络

典型的神经网络架构由输入层![](img/tex.gif)，输出层，![](img/tex1.gif)和多个隐藏层的堆栈![](img/tex2.gif)组成，其中每层由多个单元或单元组成，如图2.1。通常，每个隐藏单元![](img/tex3.gif)接收来自前一层的所有单元的输入，并定义为输入的加权组合，后跟非线性根据

![](img/tex4.gif)（2.1）

其中，![](img/tex5.gif)是控制输入单元和隐藏单元之间连接强度的权重，![](img/tex6.gif)是隐藏单元的小偏差，![](img/tex7.gif)是一些饱和非线性，如S形。

![](img/x1.png)

图2.1：典型神经网络架构的图示。图[〜]复制。

深度神经网络可以看作是Rosenblatt的感知器[122]和多层感知器[123]的现代实例。虽然，神经网络模型已存在多年（_，即_。自1960年代以来），它们直到最近才被大量使用。这种延迟有很多原因。最初的负面结果显示感知器无法对XOR这样的简单操作进行建模，阻碍了对感知器的进一步研究一段时间，直到它们被推广到多层[106]。此外，缺乏适当的训练算法会减慢进展，直到反向传播算法的普及[125]。然而，阻碍多层神经网络发展的更大障碍是它们依赖于非常大量的参数，这反过来意味着需要大量的训练数据和计算资源来支持参数的学习。

使用受限玻尔兹曼机器（RBM）[68]，在深度神经网络领域取得重大进展的主要贡献是分层无监督预训练。受限制的玻尔兹曼机器可以看作是两层神经网络，其限制形式只允许前馈连接。在图像识别的背景下，用于训练RBM的无监督学习方法可以归纳为三个步骤。首先，对于每个像素，![](img/tex8.gif)，并以一组随机权重，![](img/tex5.gif)和偏差![](img/tex6.gif)开始，每个单位的隐藏状态![](img/tex3.gif)以概率设置为![](img/tex9.gif)， ![](img/tex10.gif)。概率定义为

![](img/tex11.gif)（2.2）

哪里，![](img/tex12.gif)。其次，一旦基于等式2.2随机地设置了所有隐藏状态，通过以概率![](img/tex13.gif)将每个像素![](img/tex8.gif)设置为![](img/tex9.gif)来执行重建图像的尝试。第三，通过基于由给出的重建误差更新权重和偏差来校正隐藏单元

![](img/tex14.gif)（2.3）

其中![](img/tex15.gif)是学习率，![](img/tex16.gif)是像素![](img/tex8.gif)和隐藏单元![](img/tex3.gif)在一起的次数。整个过程重复![](img/tex17.gif)次或直到误差下降到预设阈值![](img/tex18.gif)。在训练一个层之后，其输出被用作层次结构中下一层的输入，该层又按照相同的过程进行训练。通常，在预训练所有网络层之后，使用梯度下降通过误差反向传播进一步对标记数据进行微调[68]。使用该分层无监督预训练允许训练深度神经网络而不需要大量标记数据，因为无监督RBM预训练提供了用于经验上有用的初始化各种网络参数的方式。

依赖于堆叠RBM的神经网络首先成功地部署为一种降维方法，并应用于人脸识别[69]，其中它们被用作一种自动编码器。简而言之，自动编码器可以定义为由两个主要部分组成的多层神经网络：首先，编码器将输入数据转换为特征向量;第二，解码器将生成的特征向量映射回输入空间;见，图2.2。通过最小化输入与其重建版本之间的重建误差来学习自动编码器的参数。

![](img/x2.png)

图2.2：典型自动编码器网络的结构。图[〜]复制。

除了基于RBM的自动编码器之外，后来提出了几种类型的自动编码器。每个自动编码器都引入了一种不同的正则化方法，即使在执行不同的不变性时，也能阻止网络学习琐碎的解决方案。示例包括稀疏自动编码器（SAE）[8]，去噪自动编码器（DAE）[141,142]和压缩自动编码器（CAE）[118]。稀疏自动编码器[8]允许中间表示的大小（_，即由编码器部分生成的_。）大于输入的大小，同时通过惩罚负输出来强制稀疏。相比之下，去噪自动编码器[141,142]通过尝试从人为损坏的版本重建干净的输入来改变重建本身的目标，目的是学习强大的表示。类似地，压缩自动编码器[118]通过进一步惩罚对注入噪声最敏感的单元来构建去噪自动编码器。各种类型的自动编码器的更详细的评论可以在其他地方找到[7]。

#### 2.1.2递归神经网络

在考虑依赖于顺序输入的任务时，最成功的多层架构之一是递归神经网络（RNN）[9]。如图2.3所示，RNN可以看作是一种特殊类型的神经网络，其中每个隐藏单元从当前时间步骤观察到的数据以及前一时间步的状态中获取输入。 RNN的输出定义为

![](img/tex19.gif)（2.4）

其中![](img/tex20.gif)是一些非线性挤压函数，![](img/tex21.gif)和![](img/tex22.gif)是控制当前和过去信息的相对重要性的网络参数。

![](img/x3.png)

图2.3：标准回归神经网络的操作说明。每个RNN单元在当前时间帧![](img/tex23.gif)处获取新输入，并且从之前的时间步长![](img/tex24.gif)和单元的新输出根据（2.4）计算并且可以被馈送到另一层处理中。多层RNN。

虽然RNN看似是强大的架构，但它们的主要问题之一是它们对长期依赖性建模的能力有限。这种限制归因于由于在通过多个时间步骤传播误差时可能发生的爆炸或消失梯度导致的训练困难[9]。特别是，在训练期间，反向传播的梯度乘以从当前时间步长一直到初始时间步长的网络权重。因此，由于这种乘法累加，权重可以对传播的梯度具有非平凡的影响。如果权重很小，则梯度消失，而较大的权重导致梯度爆炸。为了解决这个难题，引入了长期短期记忆（LSTM）[70]。

LSTM是经常性网络，进一步配备存储或存储器组件，如图2.4所示，随时间累积信息。 LSTM的存储器单元被门控，以便允许从其读取信息或将信息写入其中。值得注意的是，LSTM还包含一个遗忘门，允许网络在不再需要时删除信息。 LSTM由三个不同的门（输入门，![](img/tex25.gif)，遗忘门，![](img/tex26.gif)和输出门，![](img/tex27.gif)）以及存储器单元状态![](img/tex28.gif)控制。输入门由当前输入![](img/tex23.gif)和先前状态![](img/tex24.gif)控制，定义为

![](img/tex29.gif)（2.5）

其中，![](img/tex21.gif)，![](img/tex22.gif)，![](img/tex30.gif)表示控制与输入门的连接的权重和偏置，![](img/tex20.gif)通常是S形函数。遗忘门同样被定义为

![](img/tex31.gif)（2.6）

它由相应的权重和偏差![](img/tex32.gif)，![](img/tex33.gif)，![](img/tex34.gif)控制。可以说，LSTM最重要的方面是它可以应对消失和爆炸渐变的挑战。在确定存储器单元的状态时，通过遗忘和输入门状态的相加组合来实现该能力，该状态又控制信息是否经由输出门传递到另一个单元。具体地，以两个步骤计算单元状态。首先，根据估计候选小区状态

![](img/tex35.gif)（2.7）

其中![](img/tex36.gif)通常是双曲正切。其次，最终的单元状态最终由当前估计的单元状态![](img/tex37.gif)和先前的单元状态![](img/tex38.gif)控制，由输入和忘记门调制根据

![](img/tex39.gif)（2.8）

最后，使用单元的状态以及当前和先前的输入，输出门的值和LSTM单元的输出根据

![](img/tex40.gif)（2.9）

哪里

![](img/tex41.gif)（2.10）

![](img/x4.png)

图2.4：典型LSTM单元的图示。该单元在当前时间输入![](img/tex23.gif)，并从之前的时间![](img/tex24.gif)获取输入，并返回下一次输入的输出![](img/tex42.gif)。 LSTM单元的最终输出由输入门，![](img/tex25.gif)，遗忘门，![](img/tex26.gif)和输出门![](img/tex27.gif)以及存储单元状态![](img/tex28.gif)控制，它们是定义的分别在（2.5），（2.6），（2.9）和（2.8）中。图[33]再版。

#### 2.1.3卷积网络

卷积网络（ConvNets）是一种特殊类型的神经网络，特别适用于计算机视觉应用，因为它们能够通过本地操作分层抽象表示。有两个关键的设计理念推动计算机视觉中卷积体系结构的成功。首先，ConvNets利用图像的2D结构以及邻域内的像素通常高度相关的事实。因此，ConvNets避免在所有像素单元之间使用一对一连接（_，即_。就像大多数神经网络的情况一样），有利于使用分​​组本地连接。此外，ConvNet架构依赖于功能共享，因此每个通道（或输出特征图）由在所有位置使用相同滤波器的卷积生成，如图2.5所示。与标准神经网络相比，ConvNets的这一重要特性导致了一种依赖于更少参数的架构。其次，ConvNets还引入了一个汇集步骤，该步骤提供了一定程度的平移不变性，使得架构受到位置的微小变化的影响较小。值得注意的是，由于网络感知字段的大小增加，池化还允许网络逐渐看到输入的较大部分。接收场大小的增加（加上输入分辨率的降低）允许网络在网络深度增加时表示输入的更抽象的特征。例如，对于对象识别的任务，提倡ConvNets层首先将边缘聚焦到对象的部分以最终覆盖层次结构中较高层的整个对象。

![](img/x5.png)

图2.5：标准卷积网络结构的图示。图[93]再版。

卷积网络的体系结构很大程度上受到了视觉皮层中的处理的启发，如Hubel和Wiesel [74]的开创性工作所述（在第3章中进一步讨论）。事实上，最早的卷积网络实例似乎是Fukushima的Neocognitron [49]，它也依赖于本地连接，其中每个特征图最大限度地响应特定的特征类型。 Neocognitron由一系列![](img/tex43.gif)层组成，其中每层交替出现S细胞单元![](img/tex44.gif)和复杂细胞单位![](img/tex45.gif)，它们松散地模仿生物简单和复杂细胞中发生的过程，分别如图2.6所示。简单单元单元执行类似于局部卷积的操作，然后执行整流线性单元（ReLU）非线性![](img/tex46.gif)，而复杂单元执行类似于平均合并的操作。该模型还包括一个分裂的非线性，以实现类似于当代ConvNets中的规范化的东西。

![](img/x6.png)

图2.6：Neocognitron结构图。图[49]转载。

与大多数标准ConvNet架构（_，例如_。[91,88]）相反，Neocognitron不需要标记数据进行学习，因为它是基于自组织映射设计的，通过重复学习连续层之间的局部连接一组刺激图像的演示。具体地，训练Neocognitron以学习输入特征图和简单细胞层之间的连接（简单细胞层和复杂细胞层之间的连接是预先固定的），并且学习过程可以在两个步骤中概括地概括。首先，每次在输入处呈现新的刺激时，选择最大响应它的简单细胞作为该刺激类型的代表性细胞。其次，每次响应相同的输入类型时，输入和那些代表性单元之间的连接就会得到加强。值得注意的是，简单的单元层被组织在不同的组或平面中，使得每个平面仅响应一种刺激类型（_，即_。类似于现代ConvNet架构中的特征映射）。对Neocognitron的后续扩展包括监督学习的允许[51]以及自上而下的注意机制[50]。

在最近的计算机视觉应用中部署的大多数ConvNets架构都受到LeCun在1998年提出的成功架构的启发，现在称为LeNet，用于手写识别[91]。如关键文献[77,93]所述，经典卷积网络由四个基本处理层组成：（i）卷积层，（ii）非线性或整流层，（iii）归一化层和（iv）池化层。如上所述，这些成分主要存在于Neocognitron中。 LeNet的一个关键附加功能是结合反向传播，以便相对有效地学习卷积参数。

虽然ConvNets允许优化的架构，与完全连接的神经网络相比，需要的参数要少得多，但它们的主要缺点仍然是它们严重依赖学习和标记数据。这种数据依赖性可能是直到2012年ConvNets未被广泛使用的主要原因之一，因为大型ImageNet数据集的可用性[126]和相应的计算资源使得恢复对ConvNets的兴趣成为可能[88]。 ConvNets在ImageNet上的成功引发了各种ConvNet架构的突飞猛进，并且该领域的大多数贡献仅仅基于ConvNets的基本构建块的不同变化，稍后将在2.2节中讨论。